

import os, sys, io, zipfile, urllib.request, warnings, math, time, tracemalloc
from contextlib import contextmanager
warnings.filterwarnings("ignore")

# -------------------------- CONFIG --------------------------
OUT_DIR = "qprove_phish_pp_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

SEED = 42

# Quantum / model limits
N_QUBITS    = 8          # PCA -> qubits
ALPHA       = 0.10       # conformal tail
Q_TRAIN_MAX = 600        # QSVC training cap
DATA_URL    = "https://archive.ics.uci.edu/static/public/379/website%2Bphishing.zip"

# LSTM toggle (TensorFlow optional)
ENABLE_LSTM = True   # set False to skip tf install + LSTM baseline

# -------------------------- LIGHT INSTALLER --------------------------
import importlib, subprocess
def ensure(pkg):
    try:
        importlib.import_module(pkg)
    except Exception:
        print(f"[setup] Installing {pkg} ...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", pkg])

for pkg in ["numpy","pandas","scipy","scikit-learn","matplotlib","qiskit"]:
    ensure(pkg)
if ENABLE_LSTM:
    try:
        ensure("tensorflow")
    except Exception:
        print("[warn] tensorflow install failed; LSTM will be skipped.")
        ENABLE_LSTM = False

# -------------------------- IMPORTS --------------------------
import numpy as np, pandas as pd
from scipy.io import arff

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    average_precision_score, confusion_matrix, log_loss, balanced_accuracy_score,
    cohen_kappa_score, matthews_corrcoef, roc_curve, precision_recall_curve, brier_score_loss
)
from sklearn.calibration import calibration_curve

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib import gridspec

from qiskit.circuit import QuantumCircuit
from qiskit.quantum_info import Statevector

# -------------------------- PERF (runtime + peak memory) --------------------------
@contextmanager
def trace_perf():
    tracemalloc.start()
    t0 = time.perf_counter()
    try:
        yield
    finally:
        t1 = time.perf_counter()
        cur, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        trace_perf.last = {"seconds": (t1 - t0), "peak_mb": peak / (1024 * 1024)}

trace_perf.last = {"seconds": np.nan, "peak_mb": np.nan}

perf_rows = []
def log_perf(model, stage, extra=None):
    row = {
        "Model": model,
        "Stage": stage,
        "Seconds": float(trace_perf.last["seconds"]),
        "PeakMB": float(trace_perf.last["peak_mb"]),
    }
    if extra:
        row.update(extra)
    perf_rows.append(row)

# -------------------------- UTIL --------------------------
def clamp_proba(p, eps=1e-9):
    p = np.asarray(p, float)
    if p.ndim == 1:
        p = np.clip(p, eps, 1-eps)
        return np.c_[1-p, p]
    p = np.clip(p, eps, 1-eps)
    p = p / p.sum(axis=1, keepdims=True)
    return p

def entropy_from_proba(p):
    p = clamp_proba(p)
    return -(p * np.log(p + 1e-12)).sum(axis=1, keepdims=True)

def margin_from_proba(p):
    p = clamp_proba(p)
    sp = np.sort(p, axis=1)
    return (sp[:, -1] - sp[:, -2]).reshape(-1, 1)

def nonconformity_from_proba(p):
    p = clamp_proba(p)
    return (1.0 - np.max(p, axis=1)).reshape(-1, 1)

def logit(p1):
    p1 = np.clip(np.asarray(p1), 1e-9, 1-1e-9)
    return np.log(p1/(1-p1)).reshape(-1, 1)

def specificity_tnr(y_true, yhat):
    tn, fp, fn, tp = confusion_matrix(y_true, yhat, labels=[0,1]).ravel()
    return tn / max(1, (tn + fp))

def all_metrics(y_true, proba, yhat=None):
    proba = clamp_proba(proba)
    if yhat is None:
        yhat = (proba[:,1] >= 0.5).astype(int)
    return dict(
        Accuracy        = accuracy_score(y_true, yhat),
        Precision       = precision_score(y_true, yhat, zero_division=0),
        Recall          = recall_score(y_true, yhat, zero_division=0),
        F1              = f1_score(y_true, yhat, zero_division=0),
        ROC_AUC         = roc_auc_score(y_true, proba[:,1]) if len(np.unique(y_true))>1 else np.nan,
        PR_AUC          = average_precision_score(y_true, proba[:,1]),
        Log_Loss        = log_loss(y_true, proba, labels=[0,1]),
        Balanced_Acc    = balanced_accuracy_score(y_true, yhat),
        Specificity_TNR = specificity_tnr(y_true, yhat),
        Cohen_Kappa     = cohen_kappa_score(y_true, yhat),
        MCC             = matthews_corrcoef(y_true, yhat),
        Brier_Score     = brier_score_loss(y_true, proba[:,1]),
    )

def bar_panel(ax, vals, labels, title):
    ax.bar(range(len(vals)), vals)
    ax.set_xticks(range(len(vals)))
    ax.set_xticklabels(labels, rotation=60, ha='right', fontsize=8)
    ax.set_title(title)

# -------------------------- DATA LOAD --------------------------
def dl_and_read_arffs(url: str) -> pd.DataFrame:
    zpath = os.path.join(OUT_DIR, "website_phishing.zip")
    if not os.path.exists(zpath):
        print(f"[data] Downloading {url}")
        urllib.request.urlretrieve(url, zpath)
    frames = []
    with zipfile.ZipFile(zpath, "r") as zf:
        names = [n for n in zf.namelist() if n.lower().endswith(".arff")]
        if not names:
            raise RuntimeError("No .arff files in ZIP.")
        for name in names:
            with zf.open(name, "r") as raw:
                with io.TextIOWrapper(raw, encoding="utf-8", errors="ignore", newline="") as tf:
                    data, meta = arff.loadarff(tf)
            df = pd.DataFrame(data)
            for c in df.columns:
                if df[c].dtype == object and len(df[c]):
                    v = df[c].iloc[0]
                    if isinstance(v, (bytes, bytearray)):
                        df[c] = df[c].apply(lambda x: x.decode("utf-8","ignore") if isinstance(x,(bytes,bytearray)) else x)
            frames.append(df)
    return pd.concat(frames, axis=0, ignore_index=True)

def find_label_column(df: pd.DataFrame) -> str:
    for c in ["Result","result","Class","class","Label","label","Target","target"]:
        if c in df.columns:
            return c
    return df.columns[-1]

df = dl_and_read_arffs(DATA_URL)
label_col = find_label_column(df)

def to_binary_label(s):
    try:
        return 1 if float(s) > 0 else 0
    except:
        return 1 if str(s).strip().lower() in ("1","legitimate","benign","true","yes") else 0

y = df[label_col].apply(to_binary_label).astype(int).values
X = df.drop(columns=[label_col]).copy()
for c in X.columns:
    X[c] = pd.to_numeric(X[c], errors="coerce")
X = X.fillna(0)

# 60/20/20 split
X_trv, X_te, y_trv, y_te = train_test_split(X, y, test_size=0.20, random_state=SEED, stratify=y)
X_tr,  X_ca, y_tr,  y_ca = train_test_split(X_trv, y_trv, test_size=0.25, random_state=SEED, stratify=y_trv)

# -------------------------- PREPROCESS --------------------------
with trace_perf():
    std = StandardScaler()
    Xtr_s = std.fit_transform(X_tr)
    Xca_s = std.transform(X_ca)
    Xte_s = std.transform(X_te)

    pca = PCA(n_components=N_QUBITS, random_state=SEED)
    Xtr_p = pca.fit_transform(Xtr_s)
    Xca_p = pca.transform(Xca_s)
    Xte_p = pca.transform(Xte_s)

    mins = Xtr_p.min(axis=0, keepdims=True)
    maxs = Xtr_p.max(axis=0, keepdims=True)
    rng  = np.where((maxs - mins) == 0, 1.0, (maxs - mins))

    def to_theta(Z):
        return (Z - mins)/rng * (2*np.pi)

    Xtr_th = to_theta(Xtr_p)
    Xca_th = to_theta(Xca_p)
    Xte_th = to_theta(Xte_p)

log_perf("PREPROCESS", "fit+transform",
         extra={"TrainN": int(len(X_tr)), "CalN": int(len(X_ca)), "TestN": int(len(X_te)), "N_QUBITS": int(N_QUBITS)})

# -------------------------- CLASSICAL BASELINES --------------------------
# LR
with trace_perf():
    lr = LogisticRegression(max_iter=3000, random_state=SEED).fit(Xtr_s, y_tr)
log_perf("LR", "train")
with trace_perf():
    p_ca_lr = clamp_proba(lr.predict_proba(Xca_s))
    p_te_lr = clamp_proba(lr.predict_proba(Xte_s))
log_perf("LR", "infer", extra={"TestN": int(len(X_te))})

# LinearSVM
with trace_perf():
    lsvm = LinearSVC(random_state=SEED).fit(Xtr_s, y_tr)
log_perf("LinearSVM", "train")
with trace_perf():
    s_te = lsvm.decision_function(Xte_s)
    p_te_lsvm = clamp_proba(1/(1+np.exp(-s_te)))
log_perf("LinearSVM", "infer", extra={"TestN": int(len(X_te))})

# RBF-SVM
with trace_perf():
    rbf = SVC(kernel="rbf", C=1.0, gamma="scale", probability=True, random_state=SEED).fit(Xtr_s, y_tr)
log_perf("RBF_SVM", "train")
with trace_perf():
    p_ca_rbf = clamp_proba(rbf.predict_proba(Xca_s))
    p_te_rbf = clamp_proba(rbf.predict_proba(Xte_s))
log_perf("RBF_SVM", "infer", extra={"TestN": int(len(X_te))})

# MLP
with trace_perf():
    mlp = MLPClassifier(
        hidden_layer_sizes=(128, 64),
        activation="relu",
        solver="adam",
        alpha=1e-4,
        batch_size=64,
        learning_rate_init=1e-3,
        max_iter=600,
        early_stopping=True,
        n_iter_no_change=25,
        random_state=SEED
    ).fit(Xtr_s, y_tr)
log_perf("MLP", "train")
with trace_perf():
    p_ca_mlp = clamp_proba(mlp.predict_proba(Xca_s))
    p_te_mlp = clamp_proba(mlp.predict_proba(Xte_s))
log_perf("MLP", "infer", extra={"TestN": int(len(X_te))})

# LSTM (optional)
HAS_TF = False
if ENABLE_LSTM:
    try:
        import tensorflow as tf
        from tensorflow.keras import layers, models, callbacks
        HAS_TF = True
    except Exception:
        HAS_TF = False
        print("[warn] TensorFlow not available; skipping LSTM baseline.")

def keras_proba(model, Xseq):
    p = model.predict(Xseq, verbose=0).reshape(-1)
    return clamp_proba(p)

if HAS_TF:
    Xtr_seq = Xtr_s.astype("float32").reshape((-1, Xtr_s.shape[1], 1))
    Xca_seq = Xca_s.astype("float32").reshape((-1, Xca_s.shape[1], 1))
    Xte_seq = Xte_s.astype("float32").reshape((-1, Xte_s.shape[1], 1))

    with trace_perf():
        tf.random.set_seed(SEED)
        inp = layers.Input(shape=(Xtr_seq.shape[1], 1))
        x = layers.LSTM(64, return_sequences=False)(inp)
        x = layers.Dropout(0.30)(x)
        x = layers.Dense(32, activation="relu")(x)
        x = layers.Dropout(0.20)(x)
        out = layers.Dense(1, activation="sigmoid")(x)
        lstm_model = models.Model(inp, out)
        lstm_model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
            loss="binary_crossentropy"
        )
        cb = [
            callbacks.EarlyStopping(monitor="val_loss", patience=15, restore_best_weights=True),
            callbacks.ReduceLROnPlateau(monitor="val_loss", patience=7, factor=0.5, min_lr=1e-5)
        ]
        lstm_model.fit(
            Xtr_seq, y_tr,
            validation_data=(Xca_seq, y_ca),
            epochs=200,
            batch_size=64,
            verbose=0,
            callbacks=cb
        )
    log_perf("LSTM", "train")

    with trace_perf():
        p_ca_lstm = keras_proba(lstm_model, Xca_seq)
        p_te_lstm = keras_proba(lstm_model, Xte_seq)
    log_perf("LSTM", "infer", extra={"TestN": int(len(X_te))})
else:
    p_ca_lstm = None
    p_te_lstm = None

# -------------------------- CONFORMAL ROUTING (Selective Quantum) --------------------------
scores_ca = nonconformity_from_proba(p_ca_lr).ravel()
try:
    qhat = np.quantile(scores_ca, 1-ALPHA, method="higher")
except TypeError:
    qhat = np.quantile(scores_ca, 1-ALPHA)

route_mask_ca = nonconformity_from_proba(p_ca_lr).ravel() > qhat
route_mask_te = nonconformity_from_proba(p_te_lr).ravel() > qhat

# -------------------------- QUANTUM (QSVC) --------------------------
def strat_cap_idx(y_arr, max_n, seed=SEED):
    if len(y_arr) <= max_n:
        return np.arange(len(y_arr))
    sss = StratifiedShuffleSplit(n_splits=1, train_size=max_n, random_state=seed)
    idx = np.arange(len(y_arr))
    tr, _ = next(sss.split(idx, y_arr))
    return tr

tr_cap = strat_cap_idx(y_tr, Q_TRAIN_MAX, seed=SEED)
Xtr_th_q, ytr_q = Xtr_th[tr_cap], y_tr[tr_cap]

def build_state(angles, reps=1, ent="linear"):
    qc = QuantumCircuit(N_QUBITS)
    for _ in range(reps):
        for i in range(N_QUBITS):
            qc.ry(float(angles[i]), i)
        if ent == "linear":
            for i in range(N_QUBITS-1):
                qc.cx(i, i+1)
        elif ent == "ring":
            for i in range(N_QUBITS):
                qc.cx(i, (i+1) % N_QUBITS)
    return Statevector.from_instruction(qc).data

def sv_matrix(Xtheta, reps=1, ent="linear"):
    return np.stack([build_state(x, reps=reps, ent=ent) for x in Xtheta], axis=0)

def fidelity_kernel(SA, SB):
    K = SA @ SB.conj().T
    return np.abs(K)**2

def train_qsvc_and_predict(Xtrain_theta, ytrain, Xtheta, reps=1, ent="linear", tag="QSVC"):
    with trace_perf():
        SA = sv_matrix(Xtrain_theta, reps=reps, ent=ent)
        Ktr = fidelity_kernel(SA, SA)
        svc = SVC(kernel="precomputed", probability=True, C=1.0, random_state=SEED).fit(Ktr, ytrain)
    log_perf(tag, f"train(reps={reps},ent={ent})", extra={"TrainN": int(len(ytrain))})

    with trace_perf():
        SB = sv_matrix(Xtheta, reps=reps, ent=ent)
        Kte = fidelity_kernel(SB, SA)
        proba = svc.predict_proba(Kte)
    log_perf(tag, f"infer(reps={reps},ent={ent})", extra={"N": int(len(Xtheta))})
    return clamp_proba(proba)

p_ca_q1 = train_qsvc_and_predict(Xtr_th_q, ytr_q, Xca_th, reps=1, ent="linear", tag="QSVC_lin_r1")
p_te_q1 = train_qsvc_and_predict(Xtr_th_q, ytr_q, Xte_th, reps=1, ent="linear", tag="QSVC_lin_r1")

p_ca_q2 = train_qsvc_and_predict(Xtr_th_q, ytr_q, Xca_th, reps=2, ent="linear", tag="QSVC_lin_r2")
p_te_q2 = train_qsvc_and_predict(Xtr_th_q, ytr_q, Xte_th, reps=2, ent="linear", tag="QSVC_lin_r2")

# -------------------------- PROPOSED: Q-ProVe-Phish++ (Stacked Meta) --------------------------
def fill_q_probs_for_meta(p_fallback, q_prob, mask):
    q_full = p_fallback.copy()
    if mask.any():
        q_full[mask] = q_prob[mask]
    return q_full

# Use RBF_SVM as fallback base (instead of removed LightGBM)
p_ca_q1_sel = fill_q_probs_for_meta(p_ca_rbf, p_ca_q1, route_mask_ca)
p_ca_q2_sel = fill_q_probs_for_meta(p_ca_rbf, p_ca_q2, route_mask_ca)

p_te_q1_sel = fill_q_probs_for_meta(p_te_rbf, p_te_q1, route_mask_te)
p_te_q2_sel = fill_q_probs_for_meta(p_te_rbf, p_te_q2, route_mask_te)

def meta_features(
    p_rbf, p_lr, p_mlp,
    p_lstm_or_none,  # may be None
    p_q1, p_q2,
    x_pca
):
    p_rbf = clamp_proba(p_rbf); p_lr = clamp_proba(p_lr)
    p_mlp = clamp_proba(p_mlp)
    p_q1  = clamp_proba(p_q1);  p_q2  = clamp_proba(p_q2)

    rbf = p_rbf[:,1:2]; lr = p_lr[:,1:2]; mlp = p_mlp[:,1:2]
    q1  = p_q1[:,1:2];  q2  = p_q2[:,1:2]

    feats = [
        # RBF
        rbf, logit(rbf), entropy_from_proba(p_rbf), margin_from_proba(p_rbf), nonconformity_from_proba(p_rbf),
        # LR
        lr,  logit(lr),  entropy_from_proba(p_lr),  margin_from_proba(p_lr),  nonconformity_from_proba(p_lr),
        # MLP
        mlp, logit(mlp), entropy_from_proba(p_mlp), margin_from_proba(p_mlp), nonconformity_from_proba(p_mlp),
        # Quantum
        q1, logit(q1), entropy_from_proba(p_q1), margin_from_proba(p_q1), nonconformity_from_proba(p_q1),
        q2, logit(q2), entropy_from_proba(p_q2), margin_from_proba(p_q2), nonconformity_from_proba(p_q2),

        # disagreements
        np.abs(q1 - q2),
        np.abs(rbf - lr),
        np.abs(rbf - mlp),
        np.abs(lr - mlp),
        np.abs(rbf - q1),
        np.abs(rbf - q2),

        # PCA features
        np.asarray(x_pca, float),
    ]

    if p_lstm_or_none is not None:
        p_lstm = clamp_proba(p_lstm_or_none)
        lstm = p_lstm[:,1:2]
        feats.extend([
            lstm, logit(lstm), entropy_from_proba(p_lstm), margin_from_proba(p_lstm), nonconformity_from_proba(p_lstm),
            np.abs(mlp - lstm),
            np.abs(rbf - lstm),
        ])

    return np.hstack(feats)

with trace_perf():
    F_ca = meta_features(
        p_ca_rbf, p_ca_lr, p_ca_mlp,
        p_ca_lstm if HAS_TF else None,
        p_ca_q1_sel, p_ca_q2_sel,
        Xca_p
    )
    meta = LogisticRegression(max_iter=15000, C=2.0, random_state=SEED).fit(F_ca, y_ca)
log_perf("Q-ProVe-Phish++ (PROPOSED)", "meta_train",
         extra={"CalN": int(len(X_ca)), "MetaDim": int(F_ca.shape[1]), "RoutedPct_Cal": float(route_mask_ca.mean()*100.0)})

with trace_perf():
    F_te = meta_features(
        p_te_rbf, p_te_lr, p_te_mlp,
        p_te_lstm if HAS_TF else None,
        p_te_q1_sel, p_te_q2_sel,
        Xte_p
    )
    p_proposed = clamp_proba(meta.predict_proba(F_te))
log_perf("Q-ProVe-Phish++ (PROPOSED)", "meta_infer",
         extra={"TestN": int(len(X_te)), "RoutedPct_Test": float(route_mask_te.mean()*100.0)})

# -------------------------- MODEL COLLECTION --------------------------
model_probs = {
    "Q-ProVe-Phish++ (PROPOSED)": p_proposed,
    "RBF_SVM": p_te_rbf,
    "LR": p_te_lr,
    "LinearSVM": p_te_lsvm,
    "MLP": p_te_mlp,
    "QSVC_lin_r1": p_te_q1,
    "QSVC_lin_r2": p_te_q2,
}
if HAS_TF:
    model_probs["LSTM"] = p_te_lstm

# -------------------------- LEADERBOARD --------------------------
rows = []
for name, proba in model_probs.items():
    rows.append((name, all_metrics(y_te, proba)))

leader_df = pd.DataFrame({k: pd.Series(v) for k, v in rows}).T
ordered_cols = ['Accuracy','Precision','Recall','F1','ROC_AUC','PR_AUC','Log_Loss',
                'Balanced_Acc','Specificity_TNR','Cohen_Kappa','MCC','Brier_Score']
leader_df = leader_df[ordered_cols].astype(float)
leader_sorted = leader_df.sort_values(by=["ROC_AUC","F1"], ascending=False)

leader_path = os.path.join(OUT_DIR, "leaderboard_all_metrics.csv")
leader_sorted.to_csv(leader_path)

print("\n=== Leaderboard (sorted by ROC_AUC, F1) ===")
print(leader_sorted.round(4))
print(f"\n[info] RoutedPct_TEST = {route_mask_te.mean()*100.0:.2f}%  |  ALPHA={ALPHA}  |  Q_TRAIN_MAX={Q_TRAIN_MAX}")

# -------------------------- DASHBOARDS --------------------------
# Evaluation dashboard (ROC, PR, metric bars)
fig = plt.figure(figsize=(16, 10))
gs = gridspec.GridSpec(2, 3, height_ratios=[1, 1.0], width_ratios=[1, 1, 1], wspace=0.25, hspace=0.25)

ax1 = fig.add_subplot(gs[0, 0])
for name, proba in model_probs.items():
    fpr, tpr, _ = roc_curve(y_te, clamp_proba(proba)[:,1])
    ax1.plot(fpr, tpr, lw=1.3, label=name)
ax1.plot([0,1],[0,1],'k--',lw=1)
ax1.set_title("ROC (all models)"); ax1.set_xlabel("FPR"); ax1.set_ylabel("TPR")
ax1.legend(fontsize=7, ncol=2)

ax2 = fig.add_subplot(gs[0, 1])
for name, proba in model_probs.items():
    prec, rec, _ = precision_recall_curve(y_te, clamp_proba(proba)[:,1])
    ax2.plot(rec, prec, lw=1.3, label=name)
ax2.set_title("Precision–Recall (all models)"); ax2.set_xlabel("Recall"); ax2.set_ylabel("Precision")

model_order = leader_sorted.index.tolist()
ax3 = fig.add_subplot(gs[0, 2]); bar_panel(ax3, leader_sorted["ROC_AUC"].values, model_order, "ROC_AUC (↑)")
ax4 = fig.add_subplot(gs[1, 0]); bar_panel(ax4, leader_sorted["F1"].values, model_order, "F1 (↑)")
ax5 = fig.add_subplot(gs[1, 1]); bar_panel(ax5, leader_sorted["MCC"].values, model_order, "MCC (↑)")
ax6 = fig.add_subplot(gs[1, 2]); bar_panel(ax6, leader_sorted["Balanced_Acc"].values, model_order, "Balanced Accuracy (↑)")

fig.suptitle("Q-ProVe-Phish++ — Evaluation Dashboard", fontsize=14, y=1.02)
fig.tight_layout()
eval_path = os.path.join(OUT_DIR, "evaluation_dashboard.png")
fig.savefig(eval_path, dpi=150, bbox_inches='tight')
plt.close(fig)

# Diagnostics: Calibration, DET, Decision curve (top-6 for readability)
def plot_calibration_overlay(models_dict, y_true, ax):
    for name, proba in models_dict.items():
        prob = clamp_proba(proba)[:,1]
        frac_pos, mean_pred = calibration_curve(y_true, prob, n_bins=10, strategy="uniform")
        ax.plot(mean_pred, frac_pos, marker='o', ms=3, label=name)
    ax.plot([0,1],[0,1],'k--',lw=1)
    ax.set_title("Calibration"); ax.set_xlabel("Mean predicted probability"); ax.set_ylabel("Fraction of positives")

def plot_det_overlay(models_dict, y_true, ax):
    for name, proba in models_dict.items():
        fpr, tpr, _ = roc_curve(y_true, clamp_proba(proba)[:,1])
        ax.plot(fpr, 1-tpr, lw=1.2, label=name)
    ax.set_title("DET"); ax.set_xlabel("FPR"); ax.set_ylabel("FNR")

def plot_decision_curve(models_dict, y_true, ax):
    N = len(y_true); pts = np.linspace(0.01, 0.99, 50)
    for name, proba in models_dict.items():
        p = clamp_proba(proba)[:,1]; NB=[]
        for pt in pts:
            yhat = (p >= pt).astype(int)
            TP = np.sum((yhat==1) & (y_true==1)); FP = np.sum((yhat==1) & (y_true==0))
            NB.append((TP/N) - (FP/N)*(pt/(1-pt)))
        ax.plot(pts, NB, lw=1.2, label=name)
    ax.set_title("Decision Curve"); ax.set_xlabel("Threshold probability"); ax.set_ylabel("Net Benefit")

topk = min(6, len(leader_sorted))
top_names = leader_sorted.index[:topk].tolist()
top_probs = {k: model_probs[k] for k in top_names}

fig = plt.figure(figsize=(16, 9))
gs = gridspec.GridSpec(1, 3, wspace=0.25)
axA = fig.add_subplot(gs[0,0]); axB = fig.add_subplot(gs[0,1]); axC = fig.add_subplot(gs[0,2])

plot_calibration_overlay(top_probs, y_te, axA)
plot_det_overlay(top_probs, y_te, axB)
plot_decision_curve(top_probs, y_te, axC)

axA.legend(fontsize=7); axB.legend(fontsize=7); axC.legend(fontsize=7)

fig.suptitle("Diagnostics — Calibration • DET • Decision Curve", fontsize=14, y=1.02)
fig.tight_layout()
diag_path = os.path.join(OUT_DIR, "diagnostics_dashboard.png")
fig.savefig(diag_path, dpi=150, bbox_inches='tight')
plt.close(fig)

# Policy: Gains, Lift, threshold sweeps
def gains_curve(proba, y_true):
    prob = clamp_proba(proba)[:,1]
    order = np.argsort(-prob); y_sorted = y_true[order]
    cum_pos = np.cumsum(y_sorted); total_pos = max(1, y_true.sum())
    perc_samples = np.arange(1, len(y_true)+1)/len(y_true)
    gains = cum_pos/total_pos; lift = gains/np.clip(perc_samples, 1e-9, 1)
    return perc_samples, gains, lift

def threshold_sweeps(models_dict, y_true, metric="F1"):
    ths = np.linspace(0.05, 0.95, 37); curves = {}
    for name, proba in models_dict.items():
        p = clamp_proba(proba)[:,1]; vals=[]
        for t in ths:
            yhat = (p >= t).astype(int)
            if metric == "F1":
                vals.append(f1_score(y_true, yhat, zero_division=0))
            else:
                vals.append(matthews_corrcoef(y_true, yhat))
        curves[name] = (ths, np.array(vals))
    return curves

fig = plt.figure(figsize=(16, 10))
gs = gridspec.GridSpec(2, 2, wspace=0.25, hspace=0.25)
ax1 = fig.add_subplot(gs[0,0]); ax2 = fig.add_subplot(gs[0,1]); ax3 = fig.add_subplot(gs[1,0]); ax4 = fig.add_subplot(gs[1,1])

for name, proba in top_probs.items():
    x, g, L = gains_curve(proba, y_te)
    ax1.plot(x, g, lw=1.2, label=name)
    ax2.plot(x, L, lw=1.2, label=name)
ax1.plot([0,1],[0,1],'k--',lw=1)
ax1.set_title("Cumulative Gains"); ax1.set_xlabel("% of samples"); ax1.set_ylabel("% of positives")
ax2.set_title("Lift Curve"); ax2.set_xlabel("% of samples"); ax2.set_ylabel("Lift")

curves_f1  = threshold_sweeps(top_probs, y_te, metric="F1")
curves_mcc = threshold_sweeps(top_probs, y_te, metric="MCC")
for name,(ths,vals) in curves_f1.items():  ax3.plot(ths, vals, lw=1.2, label=name)
for name,(ths,vals) in curves_mcc.items(): ax4.plot(ths, vals, lw=1.2, label=name)
ax3.set_title("F1 vs Threshold"); ax3.set_xlabel("Threshold"); ax3.set_ylabel("F1")
ax4.set_title("MCC vs Threshold"); ax4.set_xlabel("Threshold"); ax4.set_ylabel("MCC")

ax1.legend(fontsize=7); ax2.legend(fontsize=7); ax3.legend(fontsize=7); ax4.legend(fontsize=7)
fig.suptitle("Policy — Gains • Lift • Threshold Sweeps", fontsize=14, y=1.02)
fig.tight_layout()
pol_path = os.path.join(OUT_DIR, "policy_dashboard.png")
fig.savefig(pol_path, dpi=150, bbox_inches='tight')
plt.close(fig)

# Confusion matrices (all models) + CSV
records = []; yhats = {}
for name, proba in model_probs.items():
    p = clamp_proba(proba)[:, 1]
    yhat = (p >= 0.5).astype(int)
    yhats[name] = yhat
    tn, fp, fn, tp = confusion_matrix(y_te, yhat, labels=[0, 1]).ravel()
    records.append({"Model": name, "TN": tn, "FP": fp, "FN": fn, "TP": tp})
cm_df = pd.DataFrame(records).set_index("Model")
cm_csv_path = os.path.join(OUT_DIR, "confusion_matrices.csv")
cm_df.to_csv(cm_csv_path)

n_models = len(model_probs); ncol = 3; nrow = int(math.ceil(n_models / ncol))
fig, axes = plt.subplots(nrow, ncol, figsize=(4.2 * ncol, 3.8 * nrow))
axes = np.array(axes).reshape(nrow, ncol)
for ax in axes.flat: ax.axis("off")

im = None
for idx, (name, yhat) in enumerate(yhats.items()):
    r, c = divmod(idx, ncol); ax = axes[r, c]; ax.axis("on")
    cm = confusion_matrix(y_te, yhat, labels=[0, 1])
    cmn = cm.astype(float) / cm.sum(axis=1, keepdims=True); cmn = np.nan_to_num(cmn)
    im = ax.imshow(cmn, vmin=0, vmax=1)
    ax.set_title(name, fontsize=9)
    ax.set_xticks([0, 1]); ax.set_yticks([0, 1])
    ax.set_xticklabels(["Phish(0)", "Legit(1)"], rotation=30, ha="right", fontsize=8)
    ax.set_yticklabels(["Phish(0)", "Legit(1)"], fontsize=8)
    for i in range(2):
        for j in range(2):
            ax.text(j, i, f"{cm[i, j]}\n({cmn[i, j]:.2f})", ha="center", va="center", fontsize=8)

cbar_ax = fig.add_axes([0.92, 0.15, 0.015, 0.7])
fig.colorbar(im, cax=cbar_ax, label="Normalized rate")
fig.suptitle("Confusion Matrices — counts (top) with normalized rates (bottom)", y=0.995)
fig.tight_layout(rect=[0, 0, 0.90, 0.97])
cm_fig_path = os.path.join(OUT_DIR, "confusion_matrices.png")
fig.savefig(cm_fig_path, dpi=150, bbox_inches="tight")
plt.close(fig)

# -------------------------- EFFICIENCY CSV --------------------------
perf_df = pd.DataFrame(perf_rows)
perf_path = os.path.join(OUT_DIR, "efficiency_runtime_memory.csv")
perf_df.to_csv(perf_path, index=False)

summary_df = (perf_df.groupby("Model")
              .agg(TotalSeconds=("Seconds","sum"), MaxPeakMB=("PeakMB","max"))
              .sort_values("TotalSeconds", ascending=True)
              .reset_index())
summary_path = os.path.join(OUT_DIR, "efficiency_runtime_memory_summary.csv")
summary_df.to_csv(summary_path, index=False)

print(f"\nSaved to: {OUT_DIR}/")
print(" -", leader_path)
print(" -", eval_path)
print(" -", diag_path)
print(" -", pol_path)
print(" -", cm_csv_path)
print(" -", cm_fig_path)
print(" -", perf_path)
print(" -", summary_path)
# ================================================================================================================
